{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Redislabs-Solution-Architects/financial-vss/blob/main/SetupColabEnvironment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2-i8jBl9GRH"
   },
   "source": [
    "# Setup Colab Environment & Data Prep\n",
    "![Redis](https://redis.com/wp-content/themes/wpx/assets/images/logo-redis.svg?auto=webp&quality=85,75&width=120)\n",
    "\n",
    "This notebook sets up the [financial-vss](https://github.com/Redislabs-Solution-Architects/financial-vss) github examples in a Google Colab runtime. It also uses [LangChain](https://python.langchain.com/docs/get_started/introduction) data loaders to prepare a dataset of PDFs for downstream indexing and semantic search tasks in Redis.\n",
    "\n",
    "Clone the full repo (if running in Colab) to get the necessary files and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'repo'...\n",
      "remote: Enumerating objects: 40, done.\u001b[K\n",
      "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
      "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
      "remote: Total 40 (delta 17), reused 30 (delta 11), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (40/40), 6.85 MiB | 1.65 MiB/s, done.\n",
      "Resolving deltas: 100% (17/17), done.\n",
      "LICENSE                      README.md\n",
      "OpenAI_LangChain_Redis.ipynb \u001b[34mresources\u001b[m\u001b[m\n",
      "OpenAI_RedisVL.ipynb\n"
     ]
    }
   ],
   "source": [
    "# If running this in Google Collab -- clone the full repo to access the contents and dataset\n",
    "!git clone https://github.com/Redislabs-Solution-Architects/financial-vss.git temp_repo\n",
    "!mv temp_repo/* .\n",
    "!rmdir temp_repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZsd9twK9-sJ"
   },
   "source": [
    "## Install Python Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C8uONNrVbe08",
    "outputId": "883b2ae1-d471-4c4a-e37a-7d286edd6d3a"
   },
   "outputs": [],
   "source": [
    "!pip install -r langchain sentence-transformers pdf2image \"unstructured[all-docs]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_h1e-L9yZfaY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from langchain.embeddings.hugging_face import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import UnstructuredFileLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KXDbABtw-vAQ"
   },
   "source": [
    "### Load text and split it into manageable chunks\n",
    "\n",
    "Without this step any large body of text would exceed the limit of tokens you can feed to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4zfOlQeeZjsN",
    "outputId": "e5d766f4-04e3-4683-bf18-1201fec84f39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['repo/resources/nke-10k-2023.pdf', 'repo/resources/amzn-10k-2023.pdf', 'repo/resources/jnj-10k-2023.pdf', 'repo/resources/aapl-10k-2023.pdf', 'repo/resources/nvd-10k-2023.pdf', 'repo/resources/msft-10k-2023.pdf']\n"
     ]
    }
   ],
   "source": [
    "# Load list of pdfs\n",
    "data_path = \"notebooks/resources/\"\n",
    "docs = [os.path.join(data_path, file) for file in os.listdir(data_path)]\n",
    "\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, we will just work with one of the 10k files. This will take some time still.\n",
    "# To Note: the UnstructuredFileLoader is not the only document loader type that LangChain provides\n",
    "# To Note: the RecursiveCharacterTextSplitter is what we use to create smaller chunks of text from the doc.\n",
    "# Docs: https://python.langchain.com/docs/integrations/document_loaders/unstructured_file\n",
    "# Docs: https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=100, add_start_index=True)\n",
    "loader = UnstructuredFileLoader(docs[0], mode=\"single\", strategy=\"fast\")\n",
    "chunks = loader.load_and_split(text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "323"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This parser pipeline broke up our PDF into smaller chunks\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content=\"NIKE, Inc.(Exact name of Registrant as specified in its charter)Oregon93-0584541(State or other jurisdiction of incorporation)(IRS Employer Identification No.)One Bowerman Drive, Beaverton, Oregon 97005-6453(Address of principal executive offices and zip code)(503) 671-6453(Registrant's telephone number, including area code)SECURITIES REGISTERED PURSUANT TO SECTION 12(B) OF THE ACT:Class B Common StockNKENew York Stock Exchange(Title of each class)(Trading symbol)(Name of each exchange on which registered)SECURITIES REGISTERED PURSUANT TO SECTION 12(G) OF THE ACT:NONE\\n\\nAs of November 30, 2022, the aggregate market values of the Registrant's Common Stock held by non-affiliates were:Class A$7,831,564,572 Class B136,467,702,472 $144,299,267,044\\n\\nTable of ContentsUNITED STATESSECURITIES AND EXCHANGE COMMISSIONWashington, D.C. 20549FORM 10-K(Mark One)☑ ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(D) OF THE SECURITIES EXCHANGE ACT OF 1934FOR THE FISCAL YEAR ENDED MAY 31, 2023OR☐ TRANSITION REPORT PURSUANT TO SECTION 13 OR 15(D) OF THE SECURITIES EXCHANGE ACT OF 1934FOR THE TRANSITION PERIOD FROM TO .Commission File No. 1-10635\\n\\nTable of Contents\\n\\nAs of July 12, 2023, the number of shares of the Registrant's Common Stock outstanding were:\\n\\nClass A\\n\\nClass B\\n\\nDOCUMENTS INCORPORATED BY REFERENCE:\\n\\nParts of Registrant's Proxy Statement for the Annual Meeting of Shareholders to be held on September 12, 2023, are incorporated by reference into Part III of this report.\\n\\n304,897,252\" metadata={'source': 'repo/resources/nke-10k-2023.pdf', 'start_index': 2361}\n"
     ]
    }
   ],
   "source": [
    "# Take a look at one item\n",
    "print(chunks[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dv1JVm31_OF9"
   },
   "source": [
    "### Initialize embeddings engine\n",
    "\n",
    "Using HuggingFace embeddings wrapper from langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "gaJrhuKa_Mwt"
   },
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed each page_content from the document chunks\n",
    "chunk_embeddings = embeddings.embed_documents([chunk.page_content for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to make sure we've created enough embeddings, 1 per document chunk\n",
    "len(chunk_embeddings) == len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Data to Disk\n",
    "Now that we have preprocessed our dataset and created semantic embddings, we will save to disk for re-use with the later notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(os.path.join(data_path, \"embeddings.json\"), \"w\") as f:\n",
    "    json.dump(chunk_embeddings, f)\n",
    "\n",
    "with open(os.path.join(data_path, \"docs.json\"), \"w\") as f:\n",
    "    json.dump([chunk.__dict__ for chunk in chunks], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to try Vector Similarity Search in Redis with three notebook options:\n",
    "\n",
    "- [Redis Python](notebooks/RedisPython_VSS.ipynb)\n",
    "- [RedisVL](notebooks/RedisVL_VSS.ipynb)\n",
    "- [LangChain](notebooks/LangChain_VSS.ipynb)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
